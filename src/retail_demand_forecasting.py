# -*- coding: utf-8 -*-
"""Retail-Demand-Forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dFv7ABxh7Us89Rxrcgg6uXeJ7yETqLNM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# PyTorch Libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
import torch.nn.functional as F

# Scikit-learn
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split

# Set random seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

class RossmannDataset(Dataset):
    """Custom PyTorch Dataset for Rossmann data"""
    def __init__(self, X, y=None):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y) if y is not None else None

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        if self.y is not None:
            return self.X[idx], self.y[idx]
        return self.X[idx]

class LSTMModel(nn.Module):
    """PyTorch LSTM Model for Sales Forecasting"""
    def __init__(self, input_size, hidden_size=128, num_layers=3, dropout=0.2):
        super(LSTMModel, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.input_size = input_size

        # LSTM layers
        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=dropout)
        self.lstm2 = nn.LSTM(hidden_size, hidden_size//2, batch_first=True, dropout=dropout)
        self.lstm3 = nn.LSTM(hidden_size//2, hidden_size//4, batch_first=True, dropout=dropout)

        # Dropout layers
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        # Batch normalization
        self.bn1 = nn.BatchNorm1d(hidden_size)
        self.bn2 = nn.BatchNorm1d(hidden_size//2)

        # Dense layers
        self.fc1 = nn.Linear(hidden_size//4, 50)
        self.fc2 = nn.Linear(50, 25)
        self.fc3 = nn.Linear(25, 1)

        # Activation
        self.relu = nn.ReLU()

    def forward(self, x):
        batch_size = x.size(0)

        # LSTM Layer 1
        lstm_out, _ = self.lstm1(x)
        # Apply batch norm on the last time step
        lstm_out_bn = self.bn1(lstm_out[:, -1, :])
        lstm_out_bn = lstm_out_bn.unsqueeze(1).repeat(1, x.size(1), 1)
        lstm_out = self.dropout1(lstm_out_bn)

        # LSTM Layer 2
        lstm_out, _ = self.lstm2(lstm_out)
        lstm_out_bn = self.bn2(lstm_out[:, -1, :])
        lstm_out_bn = lstm_out_bn.unsqueeze(1).repeat(1, x.size(1), 1)
        lstm_out = self.dropout2(lstm_out_bn)

        # LSTM Layer 3
        lstm_out, _ = self.lstm3(lstm_out)

        # Take the last output
        lstm_out = lstm_out[:, -1, :]
        lstm_out = self.dropout3(lstm_out)

        # Dense layers
        out = self.relu(self.fc1(lstm_out))
        out = self.dropout3(out)
        out = self.relu(self.fc2(out))
        out = self.fc3(out)

        return out

class RossmannLSTMForecaster:
    def __init__(self, sequence_length=30, device=None):
        """
        Initialize the Rossmann LSTM Forecaster

        Args:
            sequence_length (int): Number of past days to use for prediction
            device: PyTorch device (cuda/cpu)
        """
        self.sequence_length = sequence_length
        self.scalers = {}
        self.label_encoders = {}
        self.model = None
        self.feature_columns = []
        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")

    def load_and_preprocess_data(self, train_path, store_path, test_path=None):
        """
        Load and preprocess the Rossmann dataset
        """
        print("Loading data...")

        # Load datasets
        self.train_df = pd.read_csv(train_path)
        self.store_df = pd.read_csv(store_path)

        if test_path:
            self.test_df = pd.read_csv(test_path)

        # Merge train data with store information
        self.train_df = self.train_df.merge(self.store_df, on='Store', how='left')

        if test_path:
            self.test_df = self.test_df.merge(self.store_df, on='Store', how='left')

        print(f"Train data shape: {self.train_df.shape}")
        print(f"Store data shape: {self.store_df.shape}")

        # Convert Date column to datetime, handling errors by coercing to NaT
        self.train_df['Date'] = pd.to_datetime(self.train_df['Date'], format='%Y-%m-%d', errors='coerce')
        if test_path:
            self.test_df['Date'] = pd.to_datetime(self.test_df['Date'], format='%Y-%m-%d', errors='coerce')

        # Drop rows with NaT dates if any
        initial_rows = len(self.train_df)
        self.train_df.dropna(subset=['Date'], inplace=True)
        if len(self.train_df) < initial_rows:
            print(f"Dropped {initial_rows - len(self.train_df)} rows with invalid dates in train_df.")

        if test_path:
            initial_rows_test = len(self.test_df)
            self.test_df.dropna(subset=['Date'], inplace=True)
            if len(self.test_df) < initial_rows_test:
                print(f"Dropped {initial_rows_test - len(self.test_df)} rows with invalid dates in test_df.")


        # Sort by Store and Date
        self.train_df = self.train_df.sort_values(['Store', 'Date']).reset_index(drop=True)

        print("Data loaded and basic preprocessing completed!")
        return self.train_df.head()

    def feature_engineering(self, df):
        """
        Create time-based and other features for better forecasting
        """
        print("Creating features...")

        # Make a copy to avoid modifying original
        df = df.copy()

        # Time-based features
        df['Year'] = df['Date'].dt.year
        df['Month'] = df['Date'].dt.month
        df['Day'] = df['Date'].dt.day
        df['DayOfWeek'] = df['Date'].dt.dayofweek
        df['WeekOfYear'] = df['Date'].dt.isocalendar().week
        df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)
        df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)
        df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)

        # Cyclical features for better pattern recognition
        df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)
        df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)
        df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)
        df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)

        # Fill missing values
        df['CompetitionDistance'].fillna(df['CompetitionDistance'].median(), inplace=True)
        df['CompetitionOpenSinceMonth'].fillna(0, inplace=True)
        df['CompetitionOpenSinceYear'].fillna(0, inplace=True)

        # Promo2 features
        df['Promo2SinceWeek'].fillna(0, inplace=True)
        df['Promo2SinceYear'].fillna(0, inplace=True)
        df['PromoInterval'].fillna('None', inplace=True)

        # Competition duration feature
        df['CompetitionDuration'] = np.where(
            (df['CompetitionOpenSinceYear'] != 0) & (df['CompetitionOpenSinceMonth'] != 0),
            (df['Year'] - df['CompetitionOpenSinceYear']) * 12 +
            (df['Month'] - df['CompetitionOpenSinceMonth']),
            0
        )

        # Log transform for sales (if it exists - for training data)
        if 'Sales' in df.columns:
            # Handle zero sales
            df['Sales'] = df['Sales'].replace(0, 1)  # Replace 0 with 1 to avoid log(0)
            df['LogSales'] = np.log1p(df['Sales'])

        print("Feature engineering completed!")
        return df

    def prepare_lstm_data(self, df, target_col='LogSales'):
        """
        Prepare data for LSTM model training
        """
        print("Preparing LSTM data...")

        # Select features for the model
        feature_cols = [
            'DayOfWeek', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday',
            'Month_sin', 'Month_cos', 'DayOfWeek_sin', 'DayOfWeek_cos',
            'IsWeekend', 'IsMonthStart', 'IsMonthEnd',
            'CompetitionDistance', 'CompetitionDuration',
            'StoreType', 'Assortment'
        ]

        # Handle categorical variables
        categorical_cols = ['StateHoliday', 'StoreType', 'Assortment']

        for col in categorical_cols:
            if col not in self.label_encoders:
                self.label_encoders[col] = LabelEncoder()
                df[col] = self.label_encoders[col].fit_transform(df[col].astype(str))
            else:
                df[col] = self.label_encoders[col].transform(df[col].astype(str))

        self.feature_columns = feature_cols

        # Prepare data for each store separately
        X_list, y_list = [], []
        store_ids = []

        for store_id in df['Store'].unique():
            store_data = df[df['Store'] == store_id].copy()

            # Skip stores with insufficient data
            if len(store_data) < self.sequence_length + 1:
                continue

            # Sort by date
            store_data = store_data.sort_values('Date')

            # Prepare features and target
            features = store_data[feature_cols].values
            if target_col in store_data.columns:
                target = store_data[target_col].values
            else:
                target = None

            # Scale features
            store_scaler_key = f'store_{store_id}'
            if store_scaler_key not in self.scalers:
                self.scalers[store_scaler_key] = StandardScaler()
                features_scaled = self.scalers[store_scaler_key].fit_transform(features)
            else:
                features_scaled = self.scalers[store_scaler_key].transform(features)

            # Create sequences
            if target is not None:
                for i in range(self.sequence_length, len(features_scaled)):
                    X_list.append(features_scaled[i-self.sequence_length:i])
                    y_list.append(target[i])
                    store_ids.append(store_id)
            else:
                # For prediction data
                if len(features_scaled) >= self.sequence_length:
                    X_list.append(features_scaled[-self.sequence_length:])
                    store_ids.append(store_id)

        X = np.array(X_list)
        if y_list:
            y = np.array(y_list)
        else:
            y = None

        print(f"LSTM data prepared - X shape: {X.shape}")
        if y is not None:
            print(f"Target shape: {y.shape}")

        return X, y, store_ids

    def build_model(self, input_size, hidden_size=128, num_layers=3, dropout=0.2):
        """
        Build PyTorch LSTM model
        """
        print("Building PyTorch LSTM model...")

        self.model = LSTMModel(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout
        ).to(self.device)

        # Print model summary
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        print(f"Model built successfully!")
        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,}")

        return self.model

    def train_model(self, X_train, y_train, X_val=None, y_val=None,
                   epochs=100, batch_size=128, learning_rate=0.001):
        """
        Train the PyTorch LSTM model
        """
        print("Training PyTorch model...")

        # Create data loaders
        train_dataset = RossmannDataset(X_train, y_train)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

        if X_val is not None and y_val is not None:
            val_dataset = RossmannDataset(X_val, y_val)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        else:
            val_loader = None

        # Loss function and optimizer
        criterion = nn.MSELoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=8)

        # Training history
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 15

        for epoch in range(epochs):
            # Training phase
            self.model.train()
            train_loss = 0.0

            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)

                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()

                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                optimizer.step()
                train_loss += loss.item()

            avg_train_loss = train_loss / len(train_loader)
            train_losses.append(avg_train_loss)

            # Validation phase
            if val_loader is not None:
                self.model.eval()
                val_loss = 0.0

                with torch.no_grad():
                    for batch_X, batch_y in val_loader:
                        batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                        outputs = self.model(batch_X)
                        loss = criterion(outputs.squeeze(), batch_y)
                        val_loss += loss.item()

                avg_val_loss = val_loss / len(val_loader)
                val_losses.append(avg_val_loss)

                # Learning rate scheduling
                scheduler.step(avg_val_loss)

                # Early stopping
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    patience_counter = 0
                    # Save best model
                    torch.save(self.model.state_dict(), 'best_rossmann_model.pth')
                else:
                    patience_counter += 1

                if patience_counter >= patience:
                    print(f"Early stopping at epoch {epoch+1}")
                    break

                if (epoch + 1) % 10 == 0:
                    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')
            else:
                if (epoch + 1) % 10 == 0:
                    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}')

        # Load best model
        if val_loader is not None:
            self.model.load_state_dict(torch.load('best_rossmann_model.pth'))

        print("Model training completed!")
        return {'train_losses': train_losses, 'val_losses': val_losses}

    def evaluate_model(self, X_test, y_test, plot_results=True):
        """
        Evaluate model performance
        """
        print("Evaluating model...")

        self.model.eval()
        test_dataset = RossmannDataset(X_test, y_test)
        test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

        predictions = []
        actuals = []

        with torch.no_grad():
            for batch_X, batch_y in test_loader:
                batch_X = batch_X.to(self.device)
                outputs = self.model(batch_X)
                predictions.extend(outputs.cpu().numpy())
                actuals.extend(batch_y.numpy())

        y_pred = np.array(predictions).flatten()
        y_test = np.array(actuals)

        # Convert back from log scale
        y_test_original = np.expm1(y_test)
        y_pred_original = np.expm1(y_pred)

        # Calculate metrics
        rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))
        mae = mean_absolute_error(y_test_original, y_pred_original)

        # MAPE calculation (avoiding division by zero)
        mape = np.mean(np.abs((y_test_original - y_pred_original) /
                             np.maximum(y_test_original, 1))) * 100

        print(f"Model Performance:")
        print(f"RMSE: {rmse:.2f}")
        print(f"MAE: {mae:.2f}")
        print(f"MAPE: {mape:.2f}%")

        if plot_results:
            self.plot_predictions(y_test_original, y_pred_original)

        return {
            'rmse': rmse,
            'mae': mae,
            'mape': mape,
            'predictions': y_pred_original,
            'actual': y_test_original
        }

    def plot_predictions(self, y_true, y_pred, n_samples=1000):
        """
        Plot prediction results
        """
        plt.figure(figsize=(15, 10))

        # Limit samples for better visualization
        if len(y_true) > n_samples:
            indices = np.random.choice(len(y_true), n_samples, replace=False)
            y_true_sample = y_true[indices]
            y_pred_sample = y_pred[indices]
        else:
            y_true_sample = y_true
            y_pred_sample = y_pred

        # Plot 1: Actual vs Predicted scatter
        plt.subplot(2, 2, 1)
        plt.scatter(y_true_sample, y_pred_sample, alpha=0.5)
        plt.plot([y_true_sample.min(), y_true_sample.max()],
                [y_true_sample.min(), y_true_sample.max()], 'r--', lw=2)
        plt.xlabel('Actual Sales')
        plt.ylabel('Predicted Sales')
        plt.title('Actual vs Predicted Sales')

        # Plot 2: Time series comparison (first 200 points)
        plt.subplot(2, 2, 2)
        plt.plot(y_true_sample[:200], label='Actual', alpha=0.7)
        plt.plot(y_pred_sample[:200], label='Predicted', alpha=0.7)
        plt.xlabel('Time')
        plt.ylabel('Sales')
        plt.title('Sales Prediction Time Series')
        plt.legend()

        # Plot 3: Residuals
        plt.subplot(2, 2, 3)
        residuals = y_true_sample - y_pred_sample
        plt.hist(residuals, bins=50, alpha=0.7)
        plt.xlabel('Residuals')
        plt.ylabel('Frequency')
        plt.title('Residuals Distribution')

        # Plot 4: Residuals vs Predicted
        plt.subplot(2, 2, 4)
        plt.scatter(y_pred_sample, residuals, alpha=0.5)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('Predicted Sales')
        plt.ylabel('Residuals')
        plt.title('Residuals vs Predicted')

        plt.tight_layout()
        plt.show()

    def predict_future(self, test_data):
        """
        Make predictions on test data
        """
        print("Making predictions on test data...")

        # Feature engineering for test data
        test_data = self.feature_engineering(test_data)

        # Prepare LSTM data
        X_test, _, store_ids = self.prepare_lstm_data(test_data)

        # Make predictions
        self.model.eval()
        test_dataset = RossmannDataset(X_test)
        test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)

        predictions = []
        with torch.no_grad():
            for batch_X in test_loader:
                batch_X = batch_X.to(self.device)
                outputs = self.model(batch_X)
                predictions.extend(outputs.cpu().numpy())

        predictions = np.array(predictions).flatten()

        # Convert back from log scale
        predictions_original = np.expm1(predictions)

        # Create results dataframe
        results_df = pd.DataFrame({
            'Store': store_ids,
            'Predicted_Sales': predictions_original
        })

        return results_df

# Main function for demonstration
def main():
    """
    Main function to run the complete PyTorch LSTM forecasting pipeline
    """
    print("=== Rossmann Sales Forecasting with PyTorch LSTM ===\n")

    # Initialize forecaster
    forecaster = RossmannLSTMForecaster(sequence_length=30)

    try:
        # Load and preprocess data
        print("Loading data...")
        sample_data = forecaster.load_and_preprocess_data(
            train_path='/content/train.csv',
            store_path='/content/store.csv',
            test_path='/content/test.csv'
        )

        print("Sample of loaded data:")
        print(sample_data)

        # Feature engineering
        print("\nApplying feature engineering...")
        train_processed = forecaster.feature_engineering(forecaster.train_df)

        # Filter out closed stores and zero sales for training
        print("Filtering data for training...")
        train_processed = train_processed[
            (train_processed['Open'] == 1) & (train_processed['Sales'] > 0)
        ]

        print(f"Training data shape after filtering: {train_processed.shape}")

        # Use subset for faster training (remove this for full training)
        print("Using subset for demonstration...")
        train_processed = train_processed.sample(n=100000, random_state=42)

        # Prepare LSTM data
        print("Preparing LSTM sequences...")
        X, y, store_ids = forecaster.prepare_lstm_data(train_processed)

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        print(f"Train set: {X_train.shape}, Test set: {X_test.shape}")

        # Build model
        print("Building PyTorch LSTM model...")
        model = forecaster.build_model(input_size=X.shape[2])

        # Train model
        print("Training model (this may take a while)...")
        history = forecaster.train_model(
            X_train, y_train, X_test, y_test,
            epochs=50, batch_size=128, learning_rate=0.001
        )

        # Evaluate model
        print("Evaluating model performance...")
        results = forecaster.evaluate_model(X_test, y_test, plot_results=True)

        # Save results
        print(f"\nüìä Final Results:")
        print(f"RMSE: {results['rmse']:.2f}")
        print(f"MAE: {results['mae']:.2f}")
        print(f"MAPE: {results['mape']:.2f}%")

        # Make predictions on test set
        if hasattr(forecaster, 'test_df'):
            print("Making predictions on test data...")
            predictions = forecaster.predict_future(forecaster.test_df)

            # Save predictions
            predictions.to_csv('predictions.csv', index=False)
            print(f"‚úÖ Saved predictions for {len(predictions)} store-date combinations")

        print("\nüéâ PyTorch LSTM Pipeline completed successfully!")

    except FileNotFoundError as e:
        print(f"‚ùå Error: {e}")
        print("Make sure you have downloaded the Rossmann dataset files:")
        print("- data/train.csv")
        print("- data/store.csv")
        print("- data/test.csv")

    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        print("Check your data files and try again.")

if __name__ == "__main__":
    main()
